# =============================================================================
# Loka Inference Server — vLLM OpenAI-compatible API
# =============================================================================
# vLLM bundles torch, transformers, safetensors, and its own optimized
# attention kernels.  We only add the loka package for tool-use support
# (astro calculations, ephemeris lookups).
# =============================================================================

FROM vllm/vllm-openai:latest

USER root

# --- Loka tool-use dependencies (astro module) ------------------------------
RUN pip install --no-cache-dir \
    "astropy>=6.0,<7" \
    "jplephem>=2.18" \
    "scipy>=1.13" \
    "pydantic>=2.0"

# --- Install loka package (tools only, no training deps) --------------------
WORKDIR /code/loka
COPY . .
RUN pip install --no-cache-dir --no-deps -e .

# --- Defaults (override at runtime) -----------------------------------------
ENV VLLM_ATTENTION_BACKEND=FLASH_ATTN
ENV PYTHONUNBUFFERED=1

# Model path — mount or bake in at build time
ENV MODEL_PATH=/models

EXPOSE 8000

# vLLM's built-in OpenAI-compatible server
# Override MODEL_PATH at runtime: docker run -e MODEL_PATH=/path/to/model
CMD ["sh", "-c", "vllm serve ${MODEL_PATH} --host 0.0.0.0 --port 8000 --trust-remote-code"]
